```markdown
# âš–ï¸ Law Bot â€” A Retrieval-Augmented Generation (RAG) App

A simple RAG (Retrieval-Augmented Generation) web app that answers questions about the U.S. Constitution using a custom dataset, embeddings, and a local vector store.

Built with:
- ğŸ§  LangChain.js
- ğŸ’¬ OpenAI API
- ğŸ§± Next.js + React + Tailwind CSS

---

## ğŸš€ Features

- Ask questions about the Constitution
- Retrieves relevant chunks using vector similarity
- Uses OpenAI to generate a response with context
- Lightweight chat UI built with React and Tailwind
- Fully local (no hosted DB or backend needed)

---

## ğŸ§° Stack

| Layer       | Tech Used                     |
|-------------|-------------------------------|
| Frontend    | React (Next.js App Router)    |
| Styling     | Tailwind CSS                  |
| Backend API | Next.js API route             |
| Embedding   | OpenAI `text-embedding-ada-002` |
| LLM         | OpenAI GPT-3.5 via LangChain  |
| Vector DB   | In-memory vector store (MemoryVectorS`on in plain text, retrieved from [Project Gutenberg](https://www.gutenberg.org/ebooks/5).  
Itâ€™s pr1. Load and split the text into semantic chunks
2. Generate embeddings using OpenAI
3. Store them in memory via LangChain's `MemoryVectorStore`
4. On FAISS or Pinecone for persistent vector storage
- Add streaming / token-based generation
- Show sources / references per answer
- Deploy via Vercel orowers does Congress have?"
- "What is the Fourth A.js** (with App Router) because it lets me combine the frontend and API backend cleanly, and I'm comfortable with React. It also supports edge deployments if scaled.
- **LangChain.js**: Provides a flexible way to chain together retrieval and OpenAI calls. Their JS ecosystem made it easy to integrate with Next.js.
- **OpenAI**: I used `text-embedding-ada-002` and GPT-3.5 for fast, effective embedding + generation. No need to host models or worry about memory overhead.
- **MemoryVectorStore**: FAIï¸smooth using LangChainâ€™s `RecursiveCharacterTextSplitter`
- Switching from PDF to plain text made ingestion much simpler
- The minimal UI made it easyside API routes due to native module loading in Next.js. I solved this by switching to an in-memory store.
- `pdf-parse` caused unexpected file loadindle persistent vector storage (e.g., with Supabase or Pinecone)
- Iâ€™d add source highlighting in the response (like ChatGPTâ€™s "Cited from section X")
- Add streaming chat support for faster feedback and better UX
- Optionally swap in an open-source mas a fun and focused way to build a lightweight RAG pipeline end-to-end. It reinforced how fast modern tooling has made LLM app development. With more time, I'd love to extend this into a full legal assistant with a broader corpus and citation-aware odel (like Mistral or Llama 2) to explore local inference
                                 output.
```                                
---

## ğŸ¯ Final Thoughts

This project wHad More Time

- Iâ€™d deploy this to Vercel and hang bugs â€” I tried `pdfjs-dist` but ended up switching to a plain `.txt` version of the Constitution to keep things reliable and fast.

---

## â­ï¸ If I s

- `faiss-node` worked in ingestion but broke in to test and iterate quickly

---

## ğŸ§± Challenge What Worked Well

- Chunking and embedding were SS ran into compatibility issues inside the API route, so I swapped in a memory vector store. This works well for small datasets and demos.

---

## âš™# ğŸ”§ Tool Choices

- **Framework**: I chose **Next--

# ğŸ“ Write-Up (Submission Text)

```markdown
#] as part of a RAG app take-home challenge.
```

-mendment?"

---

## âœï¸ Author

Built by [Your Name

- "What does the First Amendment say?"
- "What p Docker

---

## ğŸ§‘â€âš–ï¸ Demo Questions

Try asking:question: retrieve similar chunks and use LLM to answer

---

## ğŸ› ï¸ Future Improvements

- Swap in eprocessed into ~500-character overlapping chunks and embedded via OpenAI.

---

## ğŸ“š How It Works

# ğŸ“„ Data

The dataset is the full U.S. Constitutip://localhost:3000](http://localhost:3000)

---

# Run the app

```bash
npm run dev
```

Visit: [htt``env
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxx
```

###
npm install
```

### Add your `.env.local` file

t//github.com/your-username/law-bot.git
cd law-bot
ore) |

---

## ğŸ“¦ Setup

```bash
git clone https:
